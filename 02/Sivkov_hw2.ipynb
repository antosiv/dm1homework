{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2 - Дерево решений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** до 27 марта 2018, 06:00   \n",
    "**Штраф за опоздание:** -2 балла после 06:00 27 марта, -4 балла после 06:00 3 апреля, -6 баллов после 06:00 10 апреля\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла   \n",
    "\n",
    "\n",
    "Присылать ДЗ необходимо в виде ссылки на свой github репозиторий в slack @alkhamush\n",
    "Необходимо в slack создать таск в приватный чат:   \n",
    "/todo Фамилия Имя *ссылка на гитхаб* @alkhamush   \n",
    "Пример:   \n",
    "/todo Ксения Стройкова https://github.com/stroykova/spheremailru/stroykova_hw2.ipynb @alkhamush   \n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Задание 1 (2 баллов)\n",
    "Разберитесь в коде MyDecisionTreeClassifier, который уже частично реализован. В комментариях, где написано \"Что делает этот блок кода?\", ответьте на этот вопрос. Допишите код там, где написано \"Ваш код\". Ваша реализация дерева должна работать по точности не хуже DecisionTreeClassifier из sklearn. Точность проверяется на wine и Speed Dating Data.\n",
    "\n",
    "###### Задание 2 (2 балла)\n",
    "Добиться скорости работы на fit сравнимой со sklearn wine и Speed Dating Data. \n",
    "Для этого используем numpy. \n",
    "\n",
    "###### Задание 3 (2 балла)\n",
    "Продемонстрируйте умение работать с Pipeline на данных Speed Dating Data и DecisionTreeClassifier. Нужно в pipeline произвести все необходимые преобразования данных и в конце обучить модель. Задание реализуйте под пунктом Задание 3 (уже написано ниже)\n",
    "\n",
    "###### Задание 4 (2 балла)\n",
    "Добавьте функционал, который определяет значения feature importance. Выведите 10 главных фичей под пунктом Задание 4 (уже написано ниже) для MyDecisionTreeClassifier и DecisionTreeClassifier так, чтобы сразу были видны выводы и по MyDecisionTreeClassifier, и по DecisionTreeClassifier. Используем данные Speed Dating Data.\n",
    "\n",
    "###### Задание 5 (2 балла)\n",
    "С помощью GridSearchCV или RandomSearchCV подберите наиболее оптимальные параметры для случайного леса (Выберете 2-3 параметра). Используем данные Speed Dating Data. Задание реализуйте под пунктом Задание 5 (уже написано ниже)\n",
    "\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст. В противном случае -1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%pycodestyle\n",
    "\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    NON_LEAF_TYPE = 0\n",
    "    LEAF_TYPE = 1\n",
    "\n",
    "    def __init__(self, min_samples_split=2, max_depth=None,\n",
    "                 sufficient_share=1.0, criterion='gini', max_features=None\n",
    "                 ):\n",
    "        self.tree = dict()\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.sufficient_share = sufficient_share\n",
    "        self.num_class = -1\n",
    "        self.feature_importances_ = None\n",
    "        if criterion == 'gini':\n",
    "            self.G_function = self.__gini\n",
    "        elif criterion == 'entropy':\n",
    "            self.G_function = self.__entropy\n",
    "        elif criterion == 'misclass':\n",
    "            self.G_function = self.__misclass\n",
    "        else:\n",
    "            print('invalid criterion name')\n",
    "            raise\n",
    "\n",
    "        if max_features == 'sqrt':\n",
    "            self.get_feature_ids = self.__get_feature_ids_sqrt\n",
    "        elif max_features == 'log2':\n",
    "            self.get_feature_ids = self.__get_feature_ids_log2\n",
    "        elif max_features is None:\n",
    "            self.get_feature_ids = self.__get_feature_ids_N\n",
    "        else:\n",
    "            print('invalid max_features name')\n",
    "            raise\n",
    "\n",
    "    def __gini(self, l_c, l_s, r_c, r_s):\n",
    "        l_s = np.float32(l_s)\n",
    "        r_s = np.float32(r_s)\n",
    "        return np.array(\n",
    "            (l_s / (l_s + r_s)) *\n",
    "            (1.0 - (np.array(l_c / l_s, dtype=np.float32) ** 2)\n",
    "             .sum(axis=-1)).reshape(-1, 1) +\n",
    "            (r_s / (l_s + r_s)) *\n",
    "            (1.0 - (np.array(r_c / r_s, dtype=np.float32) ** 2)\n",
    "             .sum(axis=-1)).reshape(-1, 1), dtype=np.float32\n",
    "                        )\n",
    "\n",
    "    def __entropy(self, l_c, l_s, r_c, r_s):\n",
    "        l_s = np.float32(l_s)\n",
    "        r_s = np.float32(r_s)\n",
    "\n",
    "        # чтобы не делать лишних вычислений\n",
    "        tmp_l = np.array(l_c / l_s, dtype=np.float32)\n",
    "        tmp_r = np.array(r_c / r_s, dtype=np.float32)\n",
    "        return -1.0 * (\n",
    "                (l_s / (l_s + r_s)) * np.nan_to_num(tmp_l * np.log(tmp_l))\n",
    "                .sum(axis=1).reshape(-1, 1) +\n",
    "                (r_s / (l_s + r_s)) * np.nan_to_num(tmp_r * np.log(tmp_r))\n",
    "                .sum(axis=1).reshape(-1, 1)\n",
    "        )\n",
    "\n",
    "    def __misclass(self, l_c, l_s, r_c, r_s):\n",
    "        l_s = np.float32(l_s)\n",
    "        r_s = np.float32(r_s)\n",
    "        return (l_s / (l_s + r_s) *\n",
    "                (1 - np.array(l_c / l_s, dtype=np.float32)\n",
    "                 .max(axis=1)).reshape(-1, 1) +\n",
    "                r_s / (l_s + r_s) *\n",
    "                (1 - np.array(r_c / r_s, dtype=np.float32)\n",
    "                 .max(axis=1)).reshape(-1, 1)\n",
    "                )\n",
    "\n",
    "    def __get_feature_ids_sqrt(self, n_feature):\n",
    "        feature_ids = np.arange(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:np.sqrt(n_feature)]\n",
    "\n",
    "    def __get_feature_ids_log2(self, n_feature):\n",
    "        feature_ids = np.arange(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:np.log2(n_feature)]\n",
    "\n",
    "    def __get_feature_ids_N(self, n_feature):\n",
    "        return np.arange(n_feature)\n",
    "\n",
    "    def __sort_samples(self, x, y):\n",
    "        sorted_idx = x.argsort()\n",
    "        return x[sorted_idx], y[sorted_idx]\n",
    "\n",
    "    def __div_samples(self, x, y, feature_id, threshold):\n",
    "        left_mask = x[:, feature_id] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        return x[left_mask], x[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def __find_threshold(self, x, y):\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "\n",
    "        # сортировка вещественного признака\n",
    "        sorted_x, sorted_y = self.__sort_samples(x, y)\n",
    "        # число уникальных классов\n",
    "        class_number = np.unique(y).shape[0]\n",
    "        # максимальный номер класса, фикс\n",
    "        # если в множестве остались только классы, например 1 и 2,\n",
    "        # то для хранения массива классов нужен\n",
    "        # массив размера 3, а не 2 (для работы индексации по номеру класса)\n",
    "        max_class = np.max(y) + 1\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "\n",
    "        # размер отрезанной части записей, фикс\n",
    "        # нужен для корректного отбрасывания записей при поиске порогов\n",
    "        # min_samples_split это минимальный размер\n",
    "        # таблицы записей необходимый для разбиения,\n",
    "        # это число не подходит для отсечения начала и конца sorted_y\n",
    "        cut_size = np.int(self.min_samples_split / 2) - 1\n",
    "        # отделение от начала и конца sorted_y cut_size элементов\n",
    "        # изменён срез для корректной обработки cut_size = 0\n",
    "        splitted_sorted_y = sorted_y[cut_size:cut_size + sorted_y.size]\n",
    "        # поиск индексов sorted_y, на которых происходит смена класса\n",
    "        # (правые границы областей с одинаковыми классами)\n",
    "        r_border_ids = np.where(splitted_sorted_y[:-1] !=\n",
    "                                splitted_sorted_y[1:])[0] + cut_size + 1\n",
    "\n",
    "        if len(r_border_ids) == 0:\n",
    "            return float('+inf'), None\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "\n",
    "        # счётчик элементов между правыми границами\n",
    "        eq_el_count = r_border_ids - np.append([cut_size], r_border_ids[:-1])\n",
    "        # создание таблицы one hot code для правых границ и классов областей,\n",
    "        # границами которых они являются\n",
    "        one_hot_code = np.zeros((r_border_ids.shape[0], max_class))\n",
    "        one_hot_code[np.arange(r_border_ids.shape[0]),\n",
    "                     sorted_y[r_border_ids - 1]] = 1\n",
    "        # новая таблица, как one_hot_code, но вместо единиц размеры областей\n",
    "        class_increments = one_hot_code * eq_el_count.reshape(-1, 1)\n",
    "        # прибавление к первой строке классов\n",
    "        # начального \"хвоста\" отсортированного признака\n",
    "        class_increments[0] = class_increments[0] + np.bincount(\n",
    "                                          sorted_y[:cut_size],\n",
    "                                          minlength=max_class\n",
    "                                          )\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "\n",
    "        # i-я строка этой таблицы - количество элементов каждого класса,\n",
    "        # находящихся слева r_border_ids[i]\n",
    "        l_class_count = np.cumsum(class_increments, axis=0)\n",
    "        # тоже самое, только справа\n",
    "        r_class_count = np.bincount(sorted_y) - l_class_count\n",
    "        # размеры частей x слева и справа от r_border_ids[i]\n",
    "        l_sizes = r_border_ids.reshape(l_class_count.shape[0], 1)\n",
    "        r_sizes = sorted_y.shape[0] - l_sizes\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "\n",
    "        # рассчёт меры информации\n",
    "        # при каждом рассматриваемом пороге для предиката\n",
    "        gs = self.G_function(l_class_count, l_sizes, r_class_count, r_sizes)\n",
    "        # порог предиката, при котором достигается минимум неопределённости\n",
    "        idx = np.argmin(gs)\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "\n",
    "        # рассчёт оптимального порога для предиката,\n",
    "        # возврат порога и метрики неопределённости\n",
    "        left_el_id = l_sizes[idx][0]\n",
    "        return gs[idx], (sorted_x[left_el_id - 1] + sorted_x[left_el_id]) / 2.0\n",
    "\n",
    "    def __fit_node(self, x, y, node_id, depth, pred_f=-1):\n",
    "        # Ваш код\n",
    "        # Необходимо использовать следующее:\n",
    "        # self.LEAF_TYPE\n",
    "        # self.NON_LEAF_TYPE\n",
    "\n",
    "        # self.tree\n",
    "        # self.max_depth\n",
    "        # self.sufficient_share\n",
    "        # self.min_samples_split\n",
    "\n",
    "        # self.get_feature_ids\n",
    "        # self.__find_threshold\n",
    "        # self.__div_samples\n",
    "        # self.__fit_node\n",
    "        if node_id == 0:\n",
    "            self.tree.clear()\n",
    "\n",
    "        if y.size == 0:\n",
    "            return\n",
    "        if np.unique(y).shape[0] == 1:\n",
    "            self.__init_leaf(x, y, node_id)\n",
    "            return\n",
    "\n",
    "        if depth == self.max_depth or \\\n",
    "                np.max(np.bincount(y)) >= \\\n",
    "                np.int64(y.size * self.sufficient_share) or \\\n",
    "                y.size < self.min_samples_split:\n",
    "            self.__init_leaf(x, y, node_id)\n",
    "            return\n",
    "\n",
    "        feature_ids = self.get_feature_ids(x.shape[1])\n",
    "\n",
    "        classes_numb = np.max(y) + 1\n",
    "        thresholds = np.zeros((2, x.shape[1]), dtype=np.float32)\n",
    "        for i in range(x.shape[1]):\n",
    "\n",
    "            if np.in1d(i, feature_ids):\n",
    "                tmp, tmp_c = np.unique(x[:, i], return_counts=True)\n",
    "                if tmp.shape[0] == 2:\n",
    "                    thresholds[1, i] = (tmp[0] + tmp[1]) / 2\n",
    "                    l_c = np.bincount(y[np.where(x[:, i] == tmp[0])],\n",
    "                                      minlength=classes_numb\n",
    "                                      )\n",
    "                    r_c = np.bincount(y) - l_c\n",
    "                    thresholds[0, i] = np.float(\n",
    "                        self.G_function(l_c, tmp_c[0], r_c, tmp_c[1])\n",
    "                    )\n",
    "                else:\n",
    "                    thresholds[:, i] = np.array(\n",
    "                        self.__find_threshold(x[:, i], y))\n",
    "\n",
    "        tmp = np.min(thresholds[0])\n",
    "        tmp_ids = np.where(thresholds[0] == tmp)[0]\n",
    "        feature_id = np.random.choice(tmp_ids)\n",
    "        threshold = thresholds[1, feature_id]\n",
    "\n",
    "        x_l, x_r, y_l, y_r = self.__div_samples(x, y, feature_id, threshold)\n",
    "\n",
    "        if y_l.size == 0 or y_r.size == 0:\n",
    "            self.__init_leaf(x, y, node_id)\n",
    "            return\n",
    "        self.__set_importance(y_l, y_r, feature_id)\n",
    "        self.__init_none_leaf(node_id, feature_id, threshold)\n",
    "        self.__fit_node(x_l, y_l, 2 * node_id + 1, depth + 1, feature_id)\n",
    "        self.__fit_node(x_r, y_r, 2 * node_id + 2, depth + 1, feature_id)\n",
    "\n",
    "    def __init_leaf(self, x, y, node_id):\n",
    "        node = []\n",
    "        node.append(self.LEAF_TYPE)\n",
    "        tmp = np.bincount(y)\n",
    "        tmp_probas = tmp / tmp.sum()\n",
    "        node.append(np.argmax(tmp))\n",
    "        node.append(tmp_probas)\n",
    "        self.tree.update({node_id: node})\n",
    "\n",
    "    def __init_none_leaf(self, node_id, feature_id, threshold):\n",
    "        node = []\n",
    "        node.append(self.NON_LEAF_TYPE)\n",
    "        node.append(feature_id)\n",
    "        node.append(threshold)\n",
    "        self.tree.update({node_id: node})\n",
    "\n",
    "    def __set_importance(self, y_l, y_r, feature_id):\n",
    "        classes_numb = np.max(np.array([np.max(np.bincount(y_l)),\n",
    "                                        np.max(np.bincount(y_r))])) + 1\n",
    "        self.feature_importances_[feature_id] += \\\n",
    "            y_l.size / (y_l.size + y_r.size) * \\\n",
    "            (1.0 - self.G_function(np.bincount(y_l, minlength=classes_numb),\n",
    "                                   y_l.size, y_r.size, y_r.size)\n",
    "             )\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.feature_importances_ = np.zeros((x.shape[1],), dtype=np.float32)\n",
    "        self.num_class = np.unique(y).size\n",
    "        self.__fit_node(x, y, 0, 0)\n",
    "\n",
    "    def __predict_class(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] <= threshold:\n",
    "                return self.__predict_class(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_class(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[1]\n",
    "\n",
    "    def __predict_probs(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] <= threshold:\n",
    "                return self.__predict_probs(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_probs(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[2]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.__predict_class(x, 0) for x in X])\n",
    "\n",
    "    def predict_probs(self, X):\n",
    "        return np.array([self.__predict_probs(x, 0) for x in X])\n",
    "\n",
    "    def fit_predict(self, x_train, y_train, predicted_x):\n",
    "        self.fit(x_train, y_train)\n",
    "        return self.predict(predicted_x)\n",
    "\n",
    "    def score(self, x, y):\n",
    "        pred_y = self.predict(x)\n",
    "        return 1.0 - (np.bincount(np.abs(y - pred_y))[1:]).sum() / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data,\n",
    "                                                    wine.target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    stratify=wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 2.6 ms, total: 2.6 ms\n",
      "Wall time: 3.49 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.3 ms, sys: 3.74 ms, total: 53 ms\n",
      "Wall time: 51.8 ms\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8857142857142858"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8857142857142858"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.002617409823292173\n"
     ]
    }
   ],
   "source": [
    "# средняя разность score sklearn дерева и score моего дерева\n",
    "# по 100 выборкам, добавил клетку. Работает несколько секунд\n",
    "tmp_sum = 0.0\n",
    "for _ in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(wine.data,\n",
    "                                                        wine.target,\n",
    "                                                        test_size=0.1,\n",
    "                                                        stratify=wine.target)\n",
    "    clf.fit(X_train, y_train)\n",
    "    my_clf.fit(X_train, y_train)\n",
    "    score1 = f1_score(y_pred=clf.predict(X_test),\n",
    "                      y_true=y_test,\n",
    "                      average='macro')\n",
    "    score2 = f1_score(y_pred=my_clf.predict(X_test),\n",
    "                      y_true=y_test,\n",
    "                      average='macro')\n",
    "    tmp_sum += score1 - score2\n",
    "print(tmp_sum / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3999, 108)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%pycodestyle\n",
    "class Cleaner(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def fit(self, df):\n",
    "        self.df = df\n",
    "        return self\n",
    "\n",
    "    def transform(self, df=None):\n",
    "        if df is None:\n",
    "            df = self.df\n",
    "        df = df.iloc[:, :97]\n",
    "        df = df.drop(['id'], axis=1)\n",
    "        df = df.drop(['idg'], axis=1)\n",
    "        df.drop_duplicates(subset=['iid']).gender.value_counts()\n",
    "        df.drop_duplicates(subset=['iid']).condtn.value_counts()\n",
    "        df = df.drop(['condtn'], axis=1)\n",
    "        df = df.drop(['round'], axis=1)\n",
    "        df = df.drop(['position', 'positin1'], axis=1)\n",
    "        df = df.drop(['order'], axis=1)\n",
    "        df = df.drop(['partner'], axis=1)\n",
    "        df = df.drop(['age_o', 'race_o', 'pf_o_att',\n",
    "                      'pf_o_sin', 'pf_o_int',\n",
    "                      'pf_o_fun', 'pf_o_amb', 'pf_o_sha',\n",
    "                      'dec_o', 'attr_o', 'sinc_o', 'intel_o', 'fun_o',\n",
    "                      'amb_o', 'shar_o', 'like_o', 'prob_o', 'met_o'],\n",
    "                     axis=1)\n",
    "        df.drop_duplicates('iid').age.isnull().sum()\n",
    "        df = df.dropna(subset=['age'])\n",
    "        df.field_cd.isnull().sum()\n",
    "        df.loc[:, 'field_cd'] = df.loc[:, 'field_cd'].fillna(0)\n",
    "        df = df.drop(['field'], axis=1)\n",
    "        df = df.drop(['undergra'], axis=1)\n",
    "        df.loc[:, 'mn_sat'] = \\\n",
    "            df.loc[:, 'mn_sat'].str.replace(',', '').astype(np.float)\n",
    "        df.loc[:, 'mn_sat'] = df.mn_sat.fillna(-999)\n",
    "        df.loc[:, 'tuition'] = \\\n",
    "            df.loc[:, 'tuition'].str.replace(',', '').astype(np.float)\n",
    "        df.loc[:, 'tuition'] = df.tuition.fillna(-999)\n",
    "        df = df.dropna(subset=['imprelig', 'imprace'])\n",
    "        df = df.drop(['from', 'zipcode'], axis=1)\n",
    "        df.loc[:, 'income'] = \\\n",
    "            df.loc[:, 'income'].str.replace(',', '').astype(np.float)\n",
    "        df.loc[:, 'income'] = df.loc[:, 'income'].fillna(-999)\n",
    "        df = df.dropna(subset=['date'])\n",
    "        df.loc[:, 'career_c'] = df.loc[:, 'career_c'].fillna(0)\n",
    "        df = df.drop(['career'], axis=1)\n",
    "        df = df.drop(['sports', 'tvsports', 'exercise', 'dining',\n",
    "                      'museums', 'art', 'hiking', 'gaming',\n",
    "                      'clubbing', 'reading', 'tv', 'theater', 'movies',\n",
    "                      'concerts', 'music', 'shopping', 'yoga'], axis=1)\n",
    "        df = df.drop(['expnum'], axis=1)\n",
    "\n",
    "        feat = ['iid', 'wave', 'attr1_1', 'sinc1_1',\n",
    "                'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']\n",
    "        temp = df.drop_duplicates(subset=['iid', 'wave']).loc[:, feat]\n",
    "        temp.loc[:, 'totalsum'] = temp.iloc[:, 2:].sum(axis=1)\n",
    "        df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr1_1', 'sinc1_1',\n",
    "                                                'intel1_1', 'fun1_1',\n",
    "                                                'amb1_1', 'shar1_1'\n",
    "                                                ]].sum(axis=1)\n",
    "        df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1',\n",
    "                   'fun1_1', 'amb1_1', 'shar1_1']] = \\\n",
    "            (df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1',\n",
    "                        'fun1_1', 'amb1_1', 'shar1_1']].T /\n",
    "             df.loc[:, 'temp_totalsum'].T).T * 100\n",
    "        feat = ['iid', 'wave', 'attr2_1', 'sinc2_1',\n",
    "                'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']\n",
    "        temp = df.drop_duplicates(subset=['iid', 'wave']).loc[:, feat]\n",
    "        temp.loc[:, 'totalsum'] = temp.iloc[:, 2:].sum(axis=1)\n",
    "        df.loc[:, 'temp_totalsum'] = df.loc[:,\n",
    "                                            ['attr2_1', 'sinc2_1', 'intel2_1',\n",
    "                                                'fun2_1', 'amb2_1', 'shar2_1'\n",
    "                                             ]\n",
    "                                            ].sum(axis=1)\n",
    "        df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1',\n",
    "                   'fun2_1', 'amb2_1', 'shar2_1']] = \\\n",
    "            (df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1',\n",
    "                        'fun2_1', 'amb2_1', 'shar2_1']].T /\n",
    "             df.loc[:, 'temp_totalsum'].T).T * 100\n",
    "        df = df.drop(['temp_totalsum'], axis=1)\n",
    "\n",
    "        for i in [4, 5]:\n",
    "            feat = ['attr{}_1'.format(i), 'sinc{}_1'.format(i),\n",
    "                    'intel{}_1'.format(i), 'fun{}_1'.format(i),\n",
    "                    'amb{}_1'.format(i), 'shar{}_1'.format(i)]\n",
    "\n",
    "            if i != 4:\n",
    "                feat.remove('shar{}_1'.format(i))\n",
    "\n",
    "            df = df.drop(feat, axis=1)\n",
    "\n",
    "        df = df.drop(['wave'], axis=1)\n",
    "\n",
    "        # кодирование\n",
    "        tmp = pd.get_dummies(df.loc[:, 'career_c'],\n",
    "                             prefix='career_class',\n",
    "                             prefix_sep='=')\n",
    "        df = pd.concat([df, tmp], axis=1)\n",
    "        tmp = pd.get_dummies(df.loc[:, 'race'], prefix='race', prefix_sep='=')\n",
    "        df = pd.concat([df, tmp], axis=1)\n",
    "\n",
    "        df_male = df.query('gender == 1') \\\n",
    "            .drop_duplicates(subset=['iid', 'pid']) \\\n",
    "            .drop(['gender'], axis=1) \\\n",
    "            .dropna()\n",
    "        df_female = df.query('gender == 0').drop_duplicates(subset=['iid']) \\\n",
    "            .drop(['gender', 'match', 'int_corr', 'samerace'], axis=1) \\\n",
    "            .dropna()\n",
    "\n",
    "        df_female.columns = df_female.columns + '_f'\n",
    "        df_female = df_female.drop(['pid_f'], axis=1)\n",
    "        df_pair = df_male.join(df_female.set_index('iid_f'),\n",
    "                               on='pid',\n",
    "                               how='inner')\n",
    "        df_pair = df_pair.drop(['iid', 'pid'], axis=1)\n",
    "        return df_pair\n",
    "\n",
    "\n",
    "df = pd.read_csv('speed-dating-experiment/Speed Dating Data.csv',\n",
    "                 encoding='cp1251')\n",
    "cleaner = Cleaner()\n",
    "cleaner.fit(df)\n",
    "df_pair = cleaner.transform()\n",
    "X = df_pair.iloc[:, 1:].values\n",
    "y = df_pair.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,\n",
    "                                                    random_state=123)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 108 ms, sys: 0 ns, total: 108 ms\n",
      "Wall time: 106 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.59 s, sys: 7.75 ms, total: 1.59 s\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на Speed Dating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5718929732433109"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07175771968494027\n"
     ]
    }
   ],
   "source": [
    "# то же самое, что и с wine_data, но с 10 выборками\n",
    "tmp_sum = 0.0\n",
    "for _ in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.1,\n",
    "                                                        random_state=123)\n",
    "    clf.fit(X_train, y_train)\n",
    "    my_clf.fit(X_train, y_train)\n",
    "    score1 = f1_score(y_pred=clf.predict(X_test),\n",
    "                      y_true=y_test, average='macro')\n",
    "    score2 = f1_score(y_pred=my_clf.predict(X_test),\n",
    "                      y_true=y_test, average='macro')\n",
    "    tmp_sum += score1 - score2\n",
    "print(tmp_sum / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6849212303075768"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('speed-dating-experiment/Speed Dating Data.csv',\n",
    "                 encoding='cp1251'\n",
    "                 )\n",
    "\n",
    "ppln_clear = Pipeline([\n",
    "    ('clean', Cleaner()),\n",
    "])\n",
    "ppln_predict = Pipeline([\n",
    "    ('clf', DecisionTreeClassifier(min_samples_split=2))\n",
    "])\n",
    "\n",
    "\n",
    "df_pair = ppln_clear.transform(df)\n",
    "X = df_pair.iloc[:, 1:].values\n",
    "y = df_pair.iloc[:, 0].values\n",
    "\n",
    "cross_val_score(ppln_predict, X, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 73 76 56 55 72 74  6  2]\n",
      "[ 0 62 72 55 18 67 71 70  7]\n"
     ]
    }
   ],
   "source": [
    "df_pair = ppln_clear.transform(df)\n",
    "X = df_pair.iloc[:, 1:].values\n",
    "y = df_pair.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.1, random_state=123)\n",
    "\n",
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "my_clf.fit(X_train, y_train)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(np.argsort(my_clf.feature_importances_)[:-10:-1])\n",
    "print(np.argsort(clf.feature_importances_)[:-10:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8375"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pair = ppln_clear.transform(df)\n",
    "X = df_pair.iloc[:, 1:].values\n",
    "y = df_pair.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.1, random_state=123)\n",
    "parameters = {'n_estimators': np.arange(5, 21),\n",
    "              'min_samples_split': np.arange(2, 8)}\n",
    "clf = GridSearchCV(RandomForestClassifier(), parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
